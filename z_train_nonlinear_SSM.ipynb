{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import wandb\n",
    "import torch\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from dataloaders.bouncing_data import BouncingBallDataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "\n",
    "from Kalman_Filter import Kalman_Filter\n",
    "from Kalman_VAE import KalmanVAE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/data2/users/lr4617/data/Bouncing_Ball/train\"\n",
    "test_dir = \"/data2/users/lr4617/data/Bouncing_Ball/test\"\n",
    "\n",
    "train_dl = BouncingBallDataLoader(train_dir, images=True)\n",
    "test_dl = BouncingBallDataLoader(test_dir, images=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = torch.linspace(-2, 2, 16)\n",
    "y_tensor = torch.linspace(2, -2, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_first_collate_fn(batch):\n",
    "    data = torch.Tensor(np.stack(batch, axis=0))\n",
    "    # data.shape: [batch size, sequence length, channels, height, width]\n",
    "    # Reshape to [sequence length, batch size, channels, height, width]\n",
    "    weight_x = data.mean(-1)\n",
    "    weight_x = (weight_x / weight_x.sum(-1).unsqueeze(-1)).squeeze(-2)\n",
    "    weight_y = data.mean(-2)\n",
    "    weight_y = (weight_y / weight_y.sum(-1).unsqueeze(-1)).squeeze(-2)\n",
    "    \n",
    "    data_x = (weight_x * x_tensor).sum(-1)\n",
    "    data_y = (weight_y * y_tensor).sum(-1)\n",
    "\n",
    "    return torch.stack([data_x, data_y], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "    train_dl,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    collate_fn=sequence_first_collate_fn,\n",
    ")\n",
    "dataloader_test = DataLoader(\n",
    "    test_dl, batch_size=128, shuffle=True, collate_fn=sequence_first_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 50\n",
    "n_channels_in = None\n",
    "dim = None\n",
    "dim_a = 2\n",
    "dim_z = 4\n",
    "K = 3\n",
    "use_MLP = True\n",
    "\n",
    "device = 0\n",
    "dtype = torch.float32\n",
    "\n",
    "# load model\n",
    "nonlinear_ssm = KalmanVAE(n_channels_in,\n",
    "                          dim,\n",
    "                          dim_a, \n",
    "                          dim_z, \n",
    "                          K, \n",
    "                          T=T, \n",
    "                          use_MLP=use_MLP,\n",
    "                          dtype=dtype, \n",
    "                          train_VAE=False).to('cuda:' + str(device)).to(dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "gamma_lr_schedule = 0.85\n",
    "\n",
    "optimizer = torch.optim.Adam(nonlinear_ssm.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma_lr_schedule)\n",
    "\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlapo0510\u001b[0m (\u001b[33minformation-theoretic-view-of-bn\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/users/lr4617/KalmanVAE/wandb/run-20231112_194251-v4dh74y2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/information-theoretic-view-of-bn/nonlinearSSM/runs/v4dh74y2' target=\"_blank\">sleek-moon-12</a></strong> to <a href='https://wandb.ai/information-theoretic-view-of-bn/nonlinearSSM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/information-theoretic-view-of-bn/nonlinearSSM' target=\"_blank\">https://wandb.ai/information-theoretic-view-of-bn/nonlinearSSM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/information-theoretic-view-of-bn/nonlinearSSM/runs/v4dh74y2' target=\"_blank\">https://wandb.ai/information-theoretic-view-of-bn/nonlinearSSM/runs/v4dh74y2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_folder = '/data2/users/lr4617/KalmanVAE/results/nonlinear_SSM/'\n",
    "\n",
    "now = datetime.now()\n",
    "run_name = 'run_' + now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "save_filename = os.path.join(output_folder, '', run_name, '')\n",
    "if not os.path.isdir(save_filename):\n",
    "    os.makedirs(save_filename)\n",
    "\n",
    "run = wandb.init(project='nonlinearSSM', \n",
    "                 config={'learning-rate': str(lr), \n",
    "                         'num_epochs': str(num_epochs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dynamics(train_loader, alpha, epoch, output_folder, dtype):\n",
    "    \n",
    "    for n, sample in enumerate(train_loader, 1):\n",
    "        if n > 1: \n",
    "            break\n",
    "        for i in range(1):\n",
    "            save_filename_sample = os.path.join(output_folder, '', 'epoch_{}'.format(epoch), 'sample_{}'.format(i))\n",
    "            if not os.path.isdir(save_filename_sample):\n",
    "                os.makedirs(save_filename_sample)\n",
    "\n",
    "            single_sample = sample[i]\n",
    "            weights = alpha[i]\n",
    "\n",
    "            for t in range(T):\n",
    "                fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "                fig.suptitle(f\"$t = {t}$\")\n",
    "                \n",
    "                axes[0].plot(single_sample[t][0], single_sample[t][1], \"o\")\n",
    "                axes[0].set_adjustable('box') \n",
    "                axes[0].set_title(r\"Observation $\\mathbf{a}_t$\")\n",
    "                axes[0].set_xlim([-2,2])\n",
    "                axes[0].set_ylim([-2,2])\n",
    "\n",
    "                axes[1].bar([\"k=0\", \"k=1\", \"k=2\"], weights[t].detach().cpu().numpy())\n",
    "                axes[1].set_title(r\"weight $\\mathbf{k}_t$\")\n",
    "\n",
    "                fig.savefig(os.path.join(save_filename_sample, 'weight-{}.png'.format(t)))\n",
    "                plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1, loss_train = 3326.27734375, time = 12.469932794570923\n",
      "epoch = 2, loss_train = 1030.6702880859375, time = 18.47687792778015\n",
      "epoch = 3, loss_train = 749.6433715820312, time = 18.250099182128906\n",
      "epoch = 4, loss_train = 611.7440795898438, time = 18.143516540527344\n",
      "epoch = 5, loss_train = 523.51025390625, time = 17.080658674240112\n",
      "epoch = 6, loss_train = 391.158935546875, time = 18.547069311141968\n"
     ]
    },
    {
     "ename": "_LinAlgError",
     "evalue": "linalg.cholesky: (Batch element 768): The factorization could not be completed because the input is not positive-definite (the leading minor of order 4 is not positive-definite).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3602927/3555711053.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mtrain_dyn_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnonlinear_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dyn_net\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dyn_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/users/lr4617/KalmanVAE/Kalman_VAE.py\u001b[0m in \u001b[0;36mcalculate_loss\u001b[0;34m(self, x, train_dyn_net, upscale_vae_loss, use_mean, recon_only)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m#### p_{gamma} (z|a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         p_z_given_a = MultivariateNormal(loc=torch.stack(smoothed_means).permute(1,0,2), \n\u001b[0;32m--> 169\u001b[0;31m                                          scale_tril=torch.linalg.cholesky(torch.stack(smoothed_covariances)).permute(1,0,2,3))\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0mz_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_z_given_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_LinAlgError\u001b[0m: linalg.cholesky: (Batch element 768): The factorization could not be completed because the input is not positive-definite (the leading minor of order 4 is not positive-definite)."
     ]
    }
   ],
   "source": [
    "dyn_save_filename = os.path.join(\n",
    "    save_filename, '', 'visualize_dynamics', '', 'training', '')\n",
    "if not os.path.isdir(dyn_save_filename):\n",
    "    os.makedirs(dyn_save_filename)\n",
    "\n",
    "start = time.time()\n",
    "log_list = []\n",
    "\n",
    "train_dyn_net = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # train\n",
    "    loss_epoch = 0.\n",
    "    idv_losses = {'LGSSM observation log likelihood': 0,\n",
    "                  'LGSSM tranisition log likelihood': 0,\n",
    "                  'LGSSM tranisition log posterior': 0}\n",
    "\n",
    "    for n, sample in enumerate(dataloader_train, 1):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sample = sample.to(dtype).to('cuda:' + str(device))\n",
    "\n",
    "        if epoch >= 5:\n",
    "            train_dyn_net = True\n",
    "\n",
    "        alpha, loss, loss_dict = nonlinear_ssm.calculate_loss(sample, train_dyn_net=train_dyn_net)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss\n",
    "\n",
    "        for key in idv_losses.keys():\n",
    "            idv_losses[key] += loss_dict[key]\n",
    "\n",
    "        alphas = alpha.detach().cpu()\n",
    "        \n",
    "    loss_epoch = loss_epoch/len(dataloader_train)\n",
    "    for key in idv_losses.keys():\n",
    "        idv_losses[key] = idv_losses[key]/len(dataloader_train)\n",
    "\n",
    "    run.log(loss_dict)\n",
    "    \n",
    "    # logistics\n",
    "    for key in idv_losses.keys():\n",
    "        idv_losses[key] = idv_losses[key]/len(dataloader_train)\n",
    "    if epoch % 20 == 0 and epoch > 0:\n",
    "        scheduler.step()\n",
    "    end = time.time()\n",
    "    log = 'epoch = {}, loss_train = {}, time = {}'.format(\n",
    "        epoch+1, loss_epoch, end-start)\n",
    "    start = end\n",
    "    print(log)\n",
    "    log_list.append(log + '\\n')\n",
    "\n",
    "    # plot dynamics\n",
    "    plot_dynamics(dataloader_train,\n",
    "                  alphas,\n",
    "                  epoch,\n",
    "                  output_folder=dyn_save_filename,\n",
    "                  dtype=dtype)\n",
    "\n",
    "    # save checkpoints\n",
    "    if epoch % 10 == 0 or epoch == num_epochs-1:\n",
    "        with open(save_filename + '/nonlinear_ssm' + str(epoch+1) + '.pt', 'wb') as f:\n",
    "            torch.save(nonlinear_ssm.state_dict(), f)\n",
    "\n",
    "    # save training log\n",
    "    with open(save_filename + '/training.cklog', \"a+\") as log_file:\n",
    "        log_file.writelines(log_list)\n",
    "        log_list.clear()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
