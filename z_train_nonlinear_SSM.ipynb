{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import wandb\n",
    "import torch\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from dataloaders.bouncing_data import BouncingBallDataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "\n",
    "from Kalman_Filter import Kalman_Filter\n",
    "from Kalman_VAE import KalmanVAE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/data2/users/lr4617/data/Bouncing_Ball/train\"\n",
    "test_dir = \"/data2/users/lr4617/data/Bouncing_Ball/test\"\n",
    "\n",
    "train_dl = BouncingBallDataLoader(train_dir, images=True)\n",
    "test_dl = BouncingBallDataLoader(test_dir, images=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = torch.linspace(-2, 2, 16)\n",
    "y_tensor = torch.linspace(2, -2, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_first_collate_fn(batch):\n",
    "    data = torch.Tensor(np.stack(batch, axis=0))\n",
    "    # data.shape: [batch size, sequence length, channels, height, width]\n",
    "    # Reshape to [sequence length, batch size, channels, height, width]\n",
    "    weight_x = data.mean(-1)\n",
    "    weight_x = (weight_x / weight_x.sum(-1).unsqueeze(-1)).squeeze(-2)\n",
    "    weight_y = data.mean(-2)\n",
    "    weight_y = (weight_y / weight_y.sum(-1).unsqueeze(-1)).squeeze(-2)\n",
    "    \n",
    "    data_x = (weight_x * x_tensor).sum(-1)\n",
    "    data_y = (weight_y * y_tensor).sum(-1)\n",
    "\n",
    "    return torch.stack([data_x, data_y], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "    train_dl,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    collate_fn=sequence_first_collate_fn,\n",
    ")\n",
    "dataloader_test = DataLoader(\n",
    "    test_dl, batch_size=128, shuffle=True, collate_fn=sequence_first_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 50\n",
    "n_channels_in = None\n",
    "dim = None\n",
    "dim_a = 2\n",
    "dim_z = 4\n",
    "K = 3\n",
    "use_MLP = True\n",
    "\n",
    "device = 0\n",
    "dtype = torch.float32\n",
    "\n",
    "# load model\n",
    "nonlinear_ssm = KalmanVAE(n_channels_in,\n",
    "                          dim,\n",
    "                          dim_a, \n",
    "                          dim_z, \n",
    "                          K, \n",
    "                          T=T, \n",
    "                          use_MLP=use_MLP,\n",
    "                          dtype=dtype, \n",
    "                          train_VAE=False).to('cuda:' + str(device)).to(dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "gamma_lr_schedule = 0.85\n",
    "\n",
    "optimizer = torch.optim.Adam(nonlinear_ssm.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma_lr_schedule)\n",
    "\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:iaaywky1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f6caa9739d44509b8c5f8dda8703cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>LGSSM observation log likelihood</td><td>█▅▄▄▅▃▇▄▄▄▁▅▃▃▅▆</td></tr><tr><td>LGSSM tranisition log likelihood</td><td>▁▃▅▆▆▆▇▇▇▇▇█████</td></tr><tr><td>LGSSM tranisition log posterior</td><td>▁▂▄▅▅▅▆▆▆▇▆█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>LGSSM observation log likelihood</td><td>111.71901</td></tr><tr><td>LGSSM tranisition log likelihood</td><td>152.24501</td></tr><tr><td>LGSSM tranisition log posterior</td><td>225.06665</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worthy-cosmos-9</strong> at: <a href='https://wandb.ai/information-theoretic-view-of-bn/nonlinearSSM/runs/iaaywky1' target=\"_blank\">https://wandb.ai/information-theoretic-view-of-bn/nonlinearSSM/runs/iaaywky1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231112_025810-iaaywky1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:iaaywky1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data2/users/lr4617/KalmanVAE/wandb/run-20231112_030334-mmdqcxqw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/information-theoretic-view-of-bn/nonlinearSSM/runs/mmdqcxqw' target=\"_blank\">neat-dew-10</a></strong> to <a href='https://wandb.ai/information-theoretic-view-of-bn/nonlinearSSM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/information-theoretic-view-of-bn/nonlinearSSM' target=\"_blank\">https://wandb.ai/information-theoretic-view-of-bn/nonlinearSSM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/information-theoretic-view-of-bn/nonlinearSSM/runs/mmdqcxqw' target=\"_blank\">https://wandb.ai/information-theoretic-view-of-bn/nonlinearSSM/runs/mmdqcxqw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_folder = '/data2/users/lr4617/KalmanVAE/results/nonlinear_SSM/'\n",
    "\n",
    "now = datetime.now()\n",
    "run_name = 'run_' + now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "save_filename = os.path.join(output_folder, '', run_name, '')\n",
    "if not os.path.isdir(save_filename):\n",
    "    os.makedirs(save_filename)\n",
    "\n",
    "run = wandb.init(project='nonlinearSSM', \n",
    "                 config={'learning-rate': str(lr), \n",
    "                         'num_epochs': str(num_epochs)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dynamics(train_loader, alpha, epoch, output_folder, dtype):\n",
    "    \n",
    "    for n, sample in enumerate(train_loader, 1):\n",
    "        if n > 1: \n",
    "            break\n",
    "        for i in range(1):\n",
    "            save_filename_sample = os.path.join(output_folder, '', 'epoch_{}'.format(epoch), 'sample_{}'.format(i))\n",
    "            if not os.path.isdir(save_filename_sample):\n",
    "                os.makedirs(save_filename_sample)\n",
    "\n",
    "            single_sample = sample[i]\n",
    "            weights = alpha[i]\n",
    "\n",
    "            for t in range(T):\n",
    "                fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "                fig.suptitle(f\"$t = {t}$\")\n",
    "                \n",
    "                axes[0].plot(single_sample[t][0], single_sample[t][1], \"o\")\n",
    "                axes[0].set_adjustable('box') \n",
    "                axes[0].set_title(r\"Observation $\\mathbf{a}_t$\")\n",
    "                axes[0].set_xlim([-2,2])\n",
    "                axes[0].set_ylim([-2,2])\n",
    "\n",
    "                axes[1].bar([\"k=0\", \"k=1\", \"k=2\"], weights[t].detach().cpu().numpy())\n",
    "                axes[1].set_title(r\"weight $\\mathbf{k}_t$\")\n",
    "\n",
    "                fig.savefig(os.path.join(save_filename_sample, 'weight-{}.png'.format(t)))\n",
    "                plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1, loss_train = 431.509033203125, time = 11.951937913894653\n",
      "epoch = 2, loss_train = 192.64378356933594, time = 18.027942895889282\n",
      "epoch = 3, loss_train = 118.2493667602539, time = 23.89493441581726\n",
      "epoch = 4, loss_train = 82.90003967285156, time = 18.820756435394287\n",
      "epoch = 5, loss_train = 61.552059173583984, time = 18.743976354599\n",
      "epoch = 6, loss_train = 42.18425369262695, time = 19.20350408554077\n",
      "epoch = 7, loss_train = 23.553030014038086, time = 18.869044542312622\n",
      "epoch = 8, loss_train = 11.670445442199707, time = 18.081520795822144\n",
      "epoch = 9, loss_train = 0.624735951423645, time = 18.433634757995605\n",
      "epoch = 10, loss_train = -7.212069034576416, time = 19.119171380996704\n",
      "epoch = 11, loss_train = -13.083212852478027, time = 17.504602193832397\n",
      "epoch = 12, loss_train = -17.617835998535156, time = 19.45293378829956\n",
      "epoch = 13, loss_train = -21.509319305419922, time = 17.565951347351074\n",
      "epoch = 14, loss_train = -24.698312759399414, time = 17.17677116394043\n",
      "epoch = 15, loss_train = -26.668649673461914, time = 20.254781246185303\n",
      "epoch = 16, loss_train = -29.011356353759766, time = 18.168846130371094\n",
      "epoch = 17, loss_train = -30.50127601623535, time = 21.377368211746216\n",
      "epoch = 18, loss_train = -32.85677719116211, time = 17.634565114974976\n",
      "epoch = 19, loss_train = -34.569820404052734, time = 17.902261972427368\n",
      "epoch = 20, loss_train = -33.14661407470703, time = 17.491087913513184\n",
      "epoch = 21, loss_train = -35.47819900512695, time = 21.77797269821167\n",
      "epoch = 22, loss_train = -36.846885681152344, time = 16.947608470916748\n",
      "epoch = 23, loss_train = -38.3260612487793, time = 16.943902730941772\n",
      "epoch = 24, loss_train = -39.253814697265625, time = 17.12214684486389\n",
      "epoch = 25, loss_train = -40.72026062011719, time = 22.179616689682007\n",
      "epoch = 26, loss_train = -41.432533264160156, time = 17.986307382583618\n",
      "epoch = 27, loss_train = -42.24197769165039, time = 17.408223152160645\n",
      "epoch = 28, loss_train = -43.27117156982422, time = 17.33206081390381\n",
      "epoch = 29, loss_train = -43.8657341003418, time = 17.710959434509277\n",
      "epoch = 30, loss_train = -44.956214904785156, time = 23.514302015304565\n",
      "epoch = 31, loss_train = -45.573726654052734, time = 17.69123411178589\n",
      "epoch = 32, loss_train = -46.01625061035156, time = 17.399688005447388\n",
      "epoch = 33, loss_train = -46.67084884643555, time = 17.00925636291504\n",
      "epoch = 34, loss_train = -46.98983383178711, time = 17.178903818130493\n",
      "epoch = 35, loss_train = -47.52305221557617, time = 17.650611400604248\n",
      "epoch = 36, loss_train = -47.683834075927734, time = 24.522595405578613\n",
      "epoch = 37, loss_train = -48.09490203857422, time = 17.88706660270691\n",
      "epoch = 38, loss_train = -48.60874938964844, time = 17.819753170013428\n",
      "epoch = 39, loss_train = -48.942161560058594, time = 17.258649587631226\n",
      "epoch = 40, loss_train = -48.91299057006836, time = 18.668177843093872\n",
      "epoch = 41, loss_train = -49.22475814819336, time = 18.39012122154236\n",
      "epoch = 42, loss_train = -49.79094314575195, time = 18.208683729171753\n",
      "epoch = 43, loss_train = -49.88690948486328, time = 26.64126706123352\n",
      "epoch = 44, loss_train = -49.904441833496094, time = 18.703150987625122\n",
      "epoch = 45, loss_train = -50.39815902709961, time = 18.554169178009033\n",
      "epoch = 46, loss_train = -50.65287399291992, time = 18.066128969192505\n",
      "epoch = 47, loss_train = -50.82088088989258, time = 17.923217296600342\n",
      "epoch = 48, loss_train = -50.92678451538086, time = 18.385255336761475\n",
      "epoch = 49, loss_train = -51.169219970703125, time = 17.644426345825195\n",
      "epoch = 50, loss_train = -51.42273712158203, time = 18.417778491973877\n"
     ]
    }
   ],
   "source": [
    "dyn_save_filename = os.path.join(\n",
    "    save_filename, '', 'visualize_dynamics', '', 'training', '')\n",
    "if not os.path.isdir(dyn_save_filename):\n",
    "    os.makedirs(dyn_save_filename)\n",
    "\n",
    "start = time.time()\n",
    "log_list = []\n",
    "\n",
    "train_dyn_net = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # train\n",
    "    loss_epoch = 0.\n",
    "    idv_losses = {'LGSSM observation log likelihood': 0,\n",
    "                  'LGSSM tranisition log likelihood': 0,\n",
    "                  'LGSSM tranisition log posterior': 0}\n",
    "\n",
    "    for n, sample in enumerate(dataloader_train, 1):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sample = sample.to(dtype).to('cuda:' + str(device))\n",
    "\n",
    "        if epoch >= 5:\n",
    "            train_dyn_net = True\n",
    "\n",
    "        alpha, loss, loss_dict = nonlinear_ssm.calculate_loss(sample, train_dyn_net=train_dyn_net)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss\n",
    "\n",
    "        for key in idv_losses.keys():\n",
    "            idv_losses[key] += loss_dict[key]\n",
    "\n",
    "        alphas = alpha.detach().cpu()\n",
    "        \n",
    "    loss_epoch = loss_epoch/len(dataloader_train)\n",
    "    for key in idv_losses.keys():\n",
    "        idv_losses[key] = idv_losses[key]/len(dataloader_train)\n",
    "\n",
    "    run.log(loss_dict)\n",
    "    \n",
    "    # logistics\n",
    "    for key in idv_losses.keys():\n",
    "        idv_losses[key] = idv_losses[key]/len(dataloader_train)\n",
    "    if epoch % 20 == 0 and epoch > 0:\n",
    "        scheduler.step()\n",
    "    end = time.time()\n",
    "    log = 'epoch = {}, loss_train = {}, time = {}'.format(\n",
    "        epoch+1, loss_epoch, end-start)\n",
    "    start = end\n",
    "    print(log)\n",
    "    log_list.append(log + '\\n')\n",
    "\n",
    "    # plot dynamics\n",
    "    plot_dynamics(dataloader_train,\n",
    "                  alphas,\n",
    "                  epoch,\n",
    "                  output_folder=dyn_save_filename,\n",
    "                  dtype=dtype)\n",
    "\n",
    "    # save checkpoints\n",
    "    if epoch % 10 == 0 or epoch == num_epochs-1:\n",
    "        with open(save_filename + '/nonlinear_ssm' + str(epoch+1) + '.pt', 'wb') as f:\n",
    "            torch.save(nonlinear_ssm.state_dict(), f)\n",
    "\n",
    "    # save training log\n",
    "    with open(save_filename + '/training.cklog', \"a+\") as log_file:\n",
    "        log_file.writelines(log_list)\n",
    "        log_list.clear()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
